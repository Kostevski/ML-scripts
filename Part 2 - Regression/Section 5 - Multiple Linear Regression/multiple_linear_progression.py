# Data Preprocessing Template# Importing the librariesimport numpy as npimport matplotlib.pyplot as pltimport pandas as pdfrom sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoderfrom sklearn.compose import ColumnTransformer# Importing the datasetdataset = pd.read_csv('50_Startups.csv')X = dataset.iloc[:, :-1].valuesy = dataset.iloc[:, 4].values #4. Encoding categorical data_________________________________________________ct = ColumnTransformer(    [('oh_enc', OneHotEncoder(sparse=False), [3])],    remainder='passthrough'  # this will add the remaining columns also)X = ct.fit_transform(X).astype('float')# Avoiding dummy trap. sklearn does this for us thoughX = X[:, 1:] # Splitting the dataset into the Training set and Test setX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)# Feature Scaling"""from sklearn.preprocessing import StandardScalersc_X = StandardScaler()X_train = sc_X.fit_transform(X_train)X_test = sc_X.transform(X_test)sc_y = StandardScaler()y_train = sc_y.fit_transform(y_train)""" # Fitting data to multilin modelfrom sklearn.linear_model import LinearRegression regressor = LinearRegression()regressor.fit(X_train, y_train)# Prediction of Test set resultsy_pred = regressor.predict(X_test) # Build the optimal model using Backwards Elemenationsimport statsmodels.formula.api as sm def backward_el(x, p):    x = np.append(arr= np.ones((len(x[:,:-1]), 1)).astype(int) , values=x, axis=1)    indexes = [x for x in range(len(x[1]))]    while True:        reg_OLS = sm.OLS(y, x[:, indexes]).fit()        p_max = max(reg_OLS.pvalues)        if p_max >= p:            del indexes[reg_OLS.pvalues.argmax()]        else:            break    del indexes[0]    corr = []    for i in indexes:        for col in dataset.columns:            if (x[:, i] == dataset[col].values).all():                string = "'{}' at index {} reached significance.".format(col, i)                corr.append(string)    return corr                p = 0.05backward_el(X, p)# ManuaallyX = np.append(arr= np.ones((50, 1)).astype(int) , values=X, axis=1)X_opt = X[:, [0, 1, 2, 3, 4, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()regressor_OLS.summary()# --> p-value highest for index[2] value so:X_opt = X[:, [0, 1, 3, 4, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()regressor_OLS.summary()# --> p-value highest for index[1] so:X_opt = X[:, [0, 3, 4, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()regressor_OLS.summary()# --> Highest p-value for index[2] so:X_opt = X[:, [0, 3, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()regressor_OLS.summary()# --> Highest p-value for index[2] so:X_opt = X[:, [0, 3]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()regressor_OLS.summary()# Conclusion. Biggest predictor for salary is X[3, R&D spend]X_opt = X[:, [0]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()regressor_OLS.summary()# -->Coeffiicent for constant is 1.12e+05 i think?## LETS TRY WITH X_TRAIN#X_test = np.append(arr= np.ones((10, 1)).astype(int) , values=X_test, axis=1)##X_opt = X_test[:, [0, 1, 2, 3, 4, 5]]#regressor_OLS = sm.OLS(endog=y_test, exog=X_opt).fit()#regressor_OLS.summary()### LETS TRY WITH X-TEST#X_opt = X_test[:, [0, 1, 3, 4, 5]]#regressor_OLS = sm.OLS(endog=y_test, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_test[:, [0, 1, 4, 5]]#regressor_OLS = sm.OLS(endog=y_test, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_test[:, [0, 1, 3]]#regressor_OLS = sm.OLS(endog=y_test, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_test[:, [0, 1]]#regressor_OLS = sm.OLS(endog=y_test, exog=X_opt).fit()#regressor_OLS.summary()### LETS TRY WITH X_TRAIN#X_train = np.append(arr= np.ones((40, 1)).astype(int) , values=X_train, axis=1)##X_opt = X_train[:, [0, 1, 2, 3, 4, 5]]#regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_train[:, [0, 1, 3, 4, 5]]#regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_train[:, [0, 3, 4, 5]]#regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_train[:, [0, 3, 4]]#regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_train[:, [0, 3]]#regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_train[:, [3]]#regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()#regressor_OLS.summary()##X_opt = X_train[:, [0]]#regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()#regressor_OLS.summary()##--> Coeff 1.094e+05